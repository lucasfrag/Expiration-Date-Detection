{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dicionário com nome das imagens e respectivas dimensões WxH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_shapes(path_to_images):\n",
    "    images_shapes = {}\n",
    "\n",
    "    images_folder = os.listdir(path_to_images)\n",
    "\n",
    "    for image_file_name in images_folder:\n",
    "        #print(image_file_name)\n",
    "        image = cv2.imread(path_to_images+image_file_name)\n",
    "        try:\n",
    "            height, width, channels = image.shape\n",
    "            #print(f'\\t{type(width)} {type(height)}')\n",
    "            images_shapes[image_file_name[:-4]] = [width, height]\n",
    "        except:\n",
    "            print('no shape info.')\n",
    "            return 0\n",
    "    \n",
    "    return images_shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conversão das datas detectadas em cada imagem: do formato yolo para coordenadas x1,y1,x2,y2 (max e min)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_yolo_labels_to_bbox(path_to_labels, image_shapes,conf_threshold=0.25): #vai retornar um dicionario \n",
    "\n",
    "    labels_bbox_dictionary = {}\n",
    "\n",
    "    labels_folder = os.listdir(path_to_labels)\n",
    "    #print(labels_folder)\n",
    "\n",
    "    for label_file_name in labels_folder:\n",
    "        # print(label_file_name)\n",
    "        if label_file_name[-4:] == '.txt':\n",
    "            with open(path_to_labels+label_file_name,'r') as label_file:\n",
    "                labels = label_file.readlines()\n",
    "                list_bbox = []\n",
    "                for label_line in labels:\n",
    "                    if label_line[0] == '1':\n",
    "\n",
    "                        # print(f'\\t{label_line}')\n",
    "                        yolo_label = label_line.split(' ')[1:]\n",
    "                        \n",
    "                        confidence = -1\n",
    "                        # print(f'\\t\\tlabel len: {len(yolo_label)}')\n",
    "                        if len(yolo_label) == 5:\n",
    "                            confidence = float(yolo_label[4])\n",
    "\n",
    "                        if confidence == -1 or confidence >= conf_threshold:\n",
    "                            \n",
    "                            # print(f'\\t\\t{confidence}')\n",
    "                            \n",
    "                            width, height = image_shapes[label_file_name[:-4]]\n",
    "                            #print(f'{width} {height}')\n",
    "                            \n",
    "                            x_center, y_center, w, h = float(yolo_label[0]), float(yolo_label[1]), float(yolo_label[2]), float(yolo_label[3])\n",
    "                            # print(f'\\t\\t{x_center}, {y_center}, {w}, {h}')\n",
    "\n",
    "                            x1 = round((x_center-w/2)*width)\n",
    "                            y1 = round((y_center-h/2)*height)\n",
    "                            x2 = round((x_center+w/2)*width)\n",
    "                            y2 = round((y_center+h/2)*height)\n",
    "                            # print(f'{x1}, {y1}, {x2}, {y2}')\n",
    "\n",
    "                            list_bbox.append([x1,y1,x2,y2])\n",
    "                        \n",
    "                labels_bbox_dictionary[label_file_name[:-4]] = list_bbox\n",
    "\n",
    "    return labels_bbox_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cálculos das métricas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funções utilizadas para validar os resultados de Mean IoU e Mean Dice feitos dentro da função 'calculate_precision_recall'\n",
    "def calculate_dice_score(boxA, boxB):\n",
    "    # Calculate intersection area\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    \n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    \n",
    "    # Calculate areas of each box\n",
    "    areaA = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    areaB = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    \n",
    "    # Calculate Dice score\n",
    "    dice_score = (2 * interArea) / (areaA + areaB) if (areaA + areaB) > 0 else 0\n",
    "    return dice_score\n",
    "\n",
    "def calculate_IoU(boxA, boxB):\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1) # utilizando +1 pois as coordenadas utilizadas estão em pixels e +1 inclui o pixel inicial no intervalo\n",
    "    \n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    \n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    return iou\n",
    "\n",
    "def calculate_mean_iou(detections, ground_truths, iou_threshold=0.5):\n",
    "    total_iou = 0\n",
    "    matched_detections = 0\n",
    "    \n",
    "    for img_id in detections.keys():\n",
    "        # Get predictions and ground truths for the image\n",
    "        predicted_boxes = detections[img_id]  # List of predicted bounding boxes\n",
    "        gt_boxes = ground_truths[img_id]      # List of ground truth bounding boxes\n",
    "        \n",
    "        for pred_box in predicted_boxes:\n",
    "            best_iou = 0\n",
    "            for gt_box in gt_boxes:\n",
    "                iou = calculate_IoU(pred_box, gt_box)\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "            \n",
    "            # If best IoU meets the threshold, count it as a match (True Positive)\n",
    "            if best_iou >= iou_threshold:\n",
    "                total_iou += best_iou\n",
    "                matched_detections += 1\n",
    "    \n",
    "    # Calculate the mean IoU\n",
    "    mean_iou = total_iou / matched_detections if matched_detections > 0 else 0\n",
    "    return mean_iou\n",
    "\n",
    "def calculate_mean_dice(detections, ground_truths, iou_threshold=0.5):\n",
    "    total_dice = 0\n",
    "    matched_detections = 0\n",
    "    \n",
    "    for img_id in detections.keys():\n",
    "        predicted_boxes = detections[img_id]  # List of predicted bounding boxes\n",
    "        gt_boxes = ground_truths[img_id]      # List of ground truth bounding boxes\n",
    "        \n",
    "        for pred_box in predicted_boxes:\n",
    "            best_dice = 0\n",
    "            for gt_box in gt_boxes:\n",
    "                iou = calculate_IoU(pred_box, gt_box)\n",
    "                \n",
    "                if iou >= iou_threshold:  # If IoU meets the threshold, calculate Dice\n",
    "                    dice = calculate_dice_score(pred_box, gt_box)\n",
    "                    if dice > best_dice:\n",
    "                        best_dice = dice\n",
    "            \n",
    "            # If the predicted box has a matching ground truth, count it as True Positive\n",
    "            if best_dice > 0:\n",
    "                total_dice += best_dice\n",
    "                matched_detections += 1\n",
    "    \n",
    "    # Calculate the mean Dice score\n",
    "    mean_dice = total_dice / matched_detections if matched_detections > 0 else 0\n",
    "    return mean_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Função para calcular todas das métricas entre 2 dicionarioa (GT vs yolo) (GT vs framework)\n",
    "def calculate_precision_recall(labels_GT, labels_predicted, iou_threashhold=0.5):\n",
    "    measures = {'TP':0, 'FN':0, 'FP':0,'TN':0}\n",
    "\n",
    "    total_iou = 0\n",
    "    total_dice = 0\n",
    "    for img_name in labels_GT:\n",
    "\n",
    "        if img_name not in labels_predicted:\n",
    "            measures['FN']+= len(labels_GT[img_name])\n",
    "            # print(f'imagem {img_name} não teve bbox detectada, mas existem {len(labels_GT[img_name])}')\n",
    "            continue\n",
    "        \n",
    "        gt_bbox_list = labels_GT[img_name]\n",
    "        pred_bbox_list = labels_predicted[img_name]\n",
    "\n",
    "        matched_gt = []\n",
    "\n",
    "        for pred_box in pred_bbox_list:\n",
    "            best_iou = 0\n",
    "            best_dice = 0\n",
    "            best_gt_box = None \n",
    "\n",
    "            for gt_box in gt_bbox_list:\n",
    "                if gt_box not in matched_gt:\n",
    "                    iou = calculate_IoU(gt_box, pred_box)\n",
    "\n",
    "                    if iou > best_iou:\n",
    "                        best_iou = iou \n",
    "                        best_gt_box = gt_box\n",
    "\n",
    "                        dice = calculate_dice_score(gt_box,pred_box)\n",
    "                        if dice > best_dice:\n",
    "                            best_dice = dice\n",
    "\n",
    "            if best_iou >= iou_threashhold:\n",
    "                measures['TP']+=1\n",
    "                matched_gt.append(best_gt_box)\n",
    "                total_iou += best_iou\n",
    "                total_dice += best_dice\n",
    "            else:\n",
    "                measures['FP']+=1\n",
    "        \n",
    "        measures['FN'] += len(gt_bbox_list) - len(matched_gt)\n",
    "\n",
    "    TP = measures['TP']\n",
    "    FP = measures['FP']\n",
    "    FN = measures['FN']\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0     # aka True Positive Rate\n",
    "    # Não é possível calcular False Positive Rate pois utiliza valores de True Negative. Influenciado pale tamanho da classe \n",
    "\n",
    "    accuracy = TP / (TP + FP + FN)\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    mean_iou = total_iou / TP if TP > 0 else 0\n",
    "    mean_dice = total_dice / TP if TP > 0 else 0\n",
    "\n",
    "    return accuracy, precision, recall, f1, measures, mean_iou, mean_dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variáveis para diretórios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2\n",
    "\n",
    "# Diretório dos Labels Ground Truth do dataset\n",
    "TEST_FOLDER = '/home/landreotti/tcc/expiry-date-detection-YOLO/datasets/Exp-Dates.v4i.yolov5pytorch/test/'\n",
    "TEST_IMAGES_FOLDER = '/home/landreotti/tcc/expiry-date-detection-YOLO/datasets/Exp-Dates.v4i.yolov5pytorch/test/images/'\n",
    "TEST_LABELS_FOLDER = '/home/landreotti/tcc/expiry-date-detection-YOLO/datasets/Exp-Dates.v4i.yolov5pytorch/test/labels/'\n",
    "\n",
    "########################## \n",
    "\n",
    "framework_predictions = '/home/landreotti/tcc/expiry-date-detection-YOLO/resultados-tcc/test/Framework/2024-11-13_experimento_datasetv4/dates_detected_bbox/'\n",
    "\n",
    "yolo_predictions_300_IoU60_Conf398 = '/home/landreotti/tcc/expiry-date-detection-YOLO/resultados-tcc/test/300/test_detect/exp47/labels/'\n",
    "\n",
    "yolo_predictions_300_DataAug_IoU60_Conf368 = '/home/landreotti/tcc/expiry-date-detection-YOLO/resultados-tcc/test/300+DataAug/test_detect/exp36/labels/'\n",
    "\n",
    "yolo_test_val_conf_thres_300_IoU60_Conf398 = '/home/landreotti/tcc/expiry-date-detection-YOLO/resultados-tcc/test/300/test_val/labels/'\n",
    "yolo_test_val_conf_thres_300_DataAug_IoU60_Conf368= \"/home/landreotti/tcc/expiry-date-detection-YOLO/resultados-tcc/test/300+DataAug/test_val/labels/\"\n",
    "\n",
    "##########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script geral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Armazenando labels ground-truth e preditos (yolo & framework) em dicionários.**\n",
    "- As chaves dos dicionários são os nomes das imagens e valor é a lista de labels (em coordenadas x1,y1,x2,y2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(predicted_labels_folder, result_name, iou_threashold=0.5, conf_threasholf=0.25):\n",
    "    \n",
    "    print(f'\\n... start results for >> {result_name} <<')\n",
    "\n",
    "    images_shapes = get_image_shapes(TEST_IMAGES_FOLDER)\n",
    "\n",
    "    labels_ground_truth = convert_yolo_labels_to_bbox(TEST_LABELS_FOLDER, images_shapes)\n",
    "    labels_predicted = convert_yolo_labels_to_bbox(predicted_labels_folder, images_shapes, conf_threasholf)\n",
    "    \n",
    "    # print(\"\\n============ Labels Ground Truth ============\")\n",
    "    # print(f'{len(labels_ground_truth)} \\t total de imagens com datas a serem detectadas') #total de imagens com datas a serem detectadas\n",
    "\n",
    "    # total_labels_gt = 0\n",
    "    # for img_name in labels_ground_truth:\n",
    "    #     total_labels_gt += len(labels_ground_truth[img_name])\n",
    "    # print(f\"{total_labels_gt} \\t total de datas a serem detectadas\")\n",
    "\n",
    "    # print(f\"\\n============ Detected by {result_name} ============\")\n",
    "\n",
    "    # print(f\"{len(labels_predicted)} \\t total de imagens onde datas foram detectadas\") #total de imagens onde datas foram detectadas\n",
    "\n",
    "    # total_labels_predicted = 0\n",
    "    # for img_name in labels_predicted:\n",
    "    #     total_labels_predicted += len(labels_predicted[img_name])\n",
    "    # print(f\"{total_labels_predicted} \\t total de datas detectadas\")\n",
    "\n",
    "\n",
    "    # print('\\n\\n\\t... calculate iou, dice, mean_iou, mean_dice')\n",
    "    # print(f'\\t{result_name}')\n",
    "    # print(f'\\t\\t{calculate_mean_iou(labels_predicted, labels_ground_truth)}')\n",
    "    # print(f'\\t\\t{calculate_mean_dice(labels_predicted, labels_ground_truth)}')\n",
    "    # print('--------------------------------------------')\n",
    "\n",
    "    print('\\n.. calculate all (accuracy, precision, recall, f1, ...)')\n",
    "    accuracy, precision, recall, f1, measures, mean_iou, mean_dice = calculate_precision_recall(labels_ground_truth, labels_predicted, iou_threashold)\n",
    "    print(f\"{'Accuracy':>10s}\\t{'Precision':>10s}\\t{'Recall':>10s}\\t{'F1':>10s}\\t{'Mean Iou':>10s}\\t{'Mean Dice':>10s}\")\n",
    "    print(f\"{accuracy*100:10.3f}\\t{precision*100:10.3f}\\t{recall*100:10.3f}\\t{f1*100:10.3f}\\t{mean_iou*100:10.3f}\\t {mean_dice*100:10.3f}\")\n",
    "    print('----------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "... start results for >> framework_predictions <<\n",
      "\n",
      ".. calculate all (accuracy, precision, recall, f1, ...)\n",
      "  Accuracy\t Precision\t    Recall\t        F1\t  Mean Iou\t Mean Dice\n",
      "    37.931\t    62.857\t    48.889\t    55.000\t    75.981\t     85.753\n",
      "----------------------------------------------------------\n",
      "\n",
      "... start results for >> yolo_predictions_300_IoU60_Conf398 <<\n",
      "\n",
      ".. calculate all (accuracy, precision, recall, f1, ...)\n",
      "  Accuracy\t Precision\t    Recall\t        F1\t  Mean Iou\t Mean Dice\n",
      "    53.097\t    72.289\t    66.667\t    69.364\t    79.425\t     88.060\n",
      "----------------------------------------------------------\n",
      "\n",
      "... start results for >> yolo_predictions_300_DataAug_IoU60_Conf368 <<\n",
      "\n",
      ".. calculate all (accuracy, precision, recall, f1, ...)\n",
      "  Accuracy\t Precision\t    Recall\t        F1\t  Mean Iou\t Mean Dice\n",
      "    63.964\t    77.174\t    78.889\t    78.022\t    82.037\t     89.698\n",
      "----------------------------------------------------------\n",
      "\n",
      "... start results for >> yolo_test_val_conf_thres_300_DataAug_IoU60_Conf368 <<\n",
      "\n",
      ".. calculate all (accuracy, precision, recall, f1, ...)\n",
      "  Accuracy\t Precision\t    Recall\t        F1\t  Mean Iou\t Mean Dice\n",
      "    63.964\t    77.174\t    78.889\t    78.022\t    81.567\t     89.432\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "get_results(framework_predictions,'framework_predictions',0.60)\n",
    "get_results(yolo_predictions_300_IoU60_Conf398,'yolo_predictions_300_IoU60_Conf398',0.6,0.398)\n",
    "get_results(yolo_predictions_300_DataAug_IoU60_Conf368,'yolo_predictions_300_DataAug_IoU60_Conf368',0.6,0.368)\n",
    "get_results(yolo_test_val_conf_thres_300_DataAug_IoU60_Conf368,'yolo_test_val_conf_thres_300_DataAug_IoU60_Conf368',0.6,0.368)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
